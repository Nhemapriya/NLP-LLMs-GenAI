{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOW+Ke6NJwvEdxfmvynat0A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Attention model\n","\n","Reference : https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n","\n","The encoder-decoder model for recurrent neural networks is an architecture for sequence-to-sequence prediction problems.\n","\n","Encoder: The encoder is responsible for stepping through the input time steps and encoding the entire sequence into a fixed length vector called a context vector.\n","\n","Decoder: The decoder is responsible for stepping through the output time steps while reading from the context vector.\n","\n","A problem with the architecture is that performance is poor on long input or output sequences.\n","\n","Attention is an extension to the architecture that addresses this limitation. It works by first providing a richer context from the encoder to the decoder and a learning mechanism where the decoder can learn where to pay attention in the richer encoding when predicting each time step in the output sequence."],"metadata":{"id":"OCNwgoEQ6Hqz"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-Wbpuih6GUq","executionInfo":{"status":"ok","timestamp":1726676008769,"user_tz":-330,"elapsed":5746,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"081c1ace-fef4-4748-c9fd-c4ab3ecb3df6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-self-attention\n","  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.26.4)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18894 sha256=6d542357a0a6deec1d0959198606f1cfe7805083be26911d41dc06b2a1a5f1b2\n","  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.51.0\n"]}],"source":["!pip install keras-self-attention"]},{"cell_type":"code","source":["from random import randint"],"metadata":{"id":"Lz66JJZK7Ros","executionInfo":{"status":"ok","timestamp":1726676694807,"user_tz":-330,"elapsed":487,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["!pip install keras-attention"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6LgZmlD6Z19","executionInfo":{"status":"ok","timestamp":1726676056748,"user_tz":-330,"elapsed":3484,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"e7e1d16c-0bdd-452a-871c-a68afbb37046"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-attention\n","  Downloading keras_attention-1.0.0-py3-none-any.whl.metadata (642 bytes)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-attention) (3.4.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (1.26.4)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (13.8.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (3.11.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (0.12.1)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (0.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras->keras-attention) (24.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-attention) (4.12.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-attention) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-attention) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-attention) (0.1.2)\n","Downloading keras_attention-1.0.0-py3-none-any.whl (7.0 kB)\n","Installing collected packages: keras-attention\n","Successfully installed keras-attention-1.0.0\n"]}]},{"cell_type":"code","source":["from numpy import array\n","from numpy import argmax\n","from numpy import array_equal\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from tensorflow.keras.layers import Attention"],"metadata":{"id":"x2Odhj186QJV","executionInfo":{"status":"ok","timestamp":1726676110075,"user_tz":-330,"elapsed":502,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"CDXluVFs9jzU","executionInfo":{"status":"ok","timestamp":1726676884616,"user_tz":-330,"elapsed":468,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# generate a sequence of random integers\n","def generate_sequence(length, n_unique):\n","\treturn [randint(0, n_unique-1) for _ in range(length)]"],"metadata":{"id":"JwtvKb9C6SSa","executionInfo":{"status":"ok","timestamp":1726676113138,"user_tz":-330,"elapsed":471,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# one hot encode sequence\n","def one_hot_encode(sequence, n_unique):\n","\tencoding = list()\n","\tfor value in sequence:\n","\t\tvector = [0 for _ in range(n_unique)]\n","\t\tvector[value] = 1\n","\t\tencoding.append(vector)\n","\treturn array(encoding)\n",""],"metadata":{"id":"J50DcAa-6T4x","executionInfo":{"status":"ok","timestamp":1726676118471,"user_tz":-330,"elapsed":546,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# decode a one hot encoded string\n","def one_hot_decode(encoded_seq):\n","\treturn [argmax(vector) for vector in encoded_seq]\n",""],"metadata":{"id":"ZOh875lI6qyz","executionInfo":{"status":"ok","timestamp":1726676121036,"user_tz":-330,"elapsed":442,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# prepare data for the LSTM\n","def get_pair(n_in, n_out, cardinality):\n","\n","\t# generate random sequence\n","\tsequence_in = generate_sequence(n_in, cardinality)\n","\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n","\n","\t# one hot encode\n","\tX = one_hot_encode(sequence_in, cardinality)\n","\ty = one_hot_encode(sequence_out, cardinality)\n","\t# reshape as 3D\n","\tX = X.reshape((1, X.shape[0], X.shape[1]))\n","\ty = y.reshape((1, y.shape[0], y.shape[1]))\n","\treturn X,y"],"metadata":{"id":"BHU00uN76s0C","executionInfo":{"status":"ok","timestamp":1726676129088,"user_tz":-330,"elapsed":449,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["n_features = 20\n","n_timesteps_in = 5\n","n_timesteps_out = 2"],"metadata":{"id":"l_tVD9LB6ual","executionInfo":{"status":"ok","timestamp":1726676135662,"user_tz":-330,"elapsed":604,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M5wk1nkH6wTx","executionInfo":{"status":"ok","timestamp":1726676705705,"user_tz":-330,"elapsed":459,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from keras.models import Model\n","from keras.layers import Input, LSTM, Dense, Attention\n","\n","# Define the input layer\n","inputs = Input(shape=(n_timesteps_in, n_features))\n","\n","# LSTM layer with return_sequences=True to output the entire sequence\n","lstm_out = LSTM(150, return_sequences=True)(inputs)\n","\n","# Attention layer\n","attention = Attention()([lstm_out, lstm_out])\n","\n","# You may need to reduce the dimensionality (optional, depending on your task)\n","dense_out = Dense(20, activation='softmax')(attention)\n","\n","# Define the model\n","model = Model(inputs=inputs, outputs=dense_out)\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"F1tibH5f7fRE","executionInfo":{"status":"ok","timestamp":1726676458080,"user_tz":-330,"elapsed":791,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"7359a61f-d5fd-40e6-b975-bd5092b8c2cc"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_5\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │        \u001b[38;5;34m120,600\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ attention_5 (\u001b[38;5;33mAttention\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n","│                           │                        │                │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │          \u001b[38;5;34m3,020\u001b[0m │ attention_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">120,600</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ attention_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n","│                           │                        │                │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,020</span> │ attention_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m123,620\u001b[0m (482.89 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">123,620</span> (482.89 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m123,620\u001b[0m (482.89 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">123,620</span> (482.89 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["X_train, y_train = [], []\n","X_test, y_test = [], []\n","for _ in range(1000):\n","    X, y = get_pair(n_timesteps_in, 3, 10)\n","    X_train.append(X)\n","    y_train.append(y)\n","\n","X_train = np.vstack(X_train)\n","y_train = np.vstack(y_train)\n"],"metadata":{"id":"DNvY8qwa60Uj","executionInfo":{"status":"ok","timestamp":1726679335946,"user_tz":-330,"elapsed":639,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from random import randint\n","from numpy import array, argmax\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense, Attention\n","from keras.utils import to_categorical\n","\n","# Generate a sequence of random integers\n","def generate_sequence(length, n_unique):\n","    return [randint(0, n_unique-1) for _ in range(length)]\n","\n","# One hot encode sequence\n","def one_hot_encode(sequence, n_unique):\n","    encoding = list()\n","    for value in sequence:\n","        vector = [0 for _ in range(n_unique)]\n","        vector[value] = 1\n","        encoding.append(vector)\n","    return array(encoding)\n","\n","# Decode a one hot encoded sequence\n","def one_hot_decode(encoded_seq):\n","    return [argmax(vector) for vector in encoded_seq]\n","\n","# Prepare data for the LSTM\n","def get_pair(n_in, n_out, cardinality):\n","    # Generate random sequence\n","    sequence_in = generate_sequence(n_in, cardinality)\n","    sequence_out = sequence_in[:n_out] + [0 for _ in range(n_in - n_out)]\n","\n","    # One hot encode\n","    X = one_hot_encode(sequence_in, cardinality)\n","    y = one_hot_encode(sequence_out, cardinality)\n","\n","    # Reshape as 3D for LSTM input\n","    X = X.reshape((1, X.shape[0], X.shape[1]))\n","    y = y.reshape((1, y.shape[0], y.shape[1]))\n","\n","    return X, y\n","\n","# Define the input shape\n","n_timesteps_in = 5  # Number of time steps in input\n","n_features = 10     # Number of unique features\n","n_out = 3           # Number of output time steps (smaller than input)\n","cardinality = 10    # Number of unique values (for one hot encoding)\n","\n","# Create the model\n","inputs = Input(shape=(n_timesteps_in, n_features))\n","\n","# LSTM layer\n","lstm_out = LSTM(150, return_sequences=True)(inputs)\n","\n","# Attention layer\n","attention = Attention()([lstm_out, lstm_out])\n","\n","# Dense output layer\n","dense_out = Dense(n_features, activation='softmax')(attention)\n","\n","# Define the model\n","model = Model(inputs=inputs, outputs=dense_out)\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Print model summary\n","model.summary()\n","\n","# Prepare training data (let's use 1000 samples)\n","X_train, y_train = [], []\n","for _ in range(1000):\n","    X, y = get_pair(n_timesteps_in, n_out, cardinality)\n","    X_train.append(X)\n","    y_train.append(y)\n","\n","# Convert to numpy arrays for Keras\n","X_train = np.vstack(X_train)\n","y_train = np.vstack(y_train)\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n","\n","# Prepare test data (let's use 100 samples for testing)\n","X_test, y_test = [], []\n","for _ in range(100):\n","    X, y = get_pair(n_timesteps_in, n_out, cardinality)\n","    X_test.append(X)\n","    y_test.append(y)\n","\n","X_test = np.vstack(X_test)\n","y_test = np.vstack(y_test)\n","\n","# Evaluate the model on test data\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n","\n","# Generate predictions on a test sample\n","X_sample, y_sample = get_pair(n_timesteps_in, n_out, cardinality)\n","y_pred = model.predict(X_sample)\n","\n","# Decode the prediction\n","decoded_input = one_hot_decode(X_sample[0])\n","decoded_output = one_hot_decode(y_sample[0])\n","decoded_pred = one_hot_decode(y_pred[0])\n","\n","print(f\"Input Sequence: {decoded_input}\")\n","print(f\"True Output Sequence: {decoded_output}\")\n","print(f\"Predicted Output Sequence: {decoded_pred}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":746},"id":"o3OtHfmLG5Mr","executionInfo":{"status":"ok","timestamp":1726679382421,"user_tz":-330,"elapsed":8782,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"a7977653-f897-4f24-9f84-c1b285421795"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_6\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m10\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │         \u001b[38;5;34m96,600\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ attention_6 (\u001b[38;5;33mAttention\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m150\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n","│                           │                        │                │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m10\u001b[0m)          │          \u001b[38;5;34m1,510\u001b[0m │ attention_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">96,600</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ attention_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n","│                           │                        │                │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,510</span> │ attention_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m98,110\u001b[0m (383.24 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,110</span> (383.24 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m98,110\u001b[0m (383.24 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,110</span> (383.24 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3737 - loss: 2.2174 - val_accuracy: 0.4540 - val_loss: 1.7169\n","Epoch 2/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4612 - loss: 1.7200 - val_accuracy: 0.4540 - val_loss: 1.6579\n","Epoch 3/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4626 - loss: 1.6209 - val_accuracy: 0.4540 - val_loss: 1.5822\n","Epoch 4/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4612 - loss: 1.5329 - val_accuracy: 0.4540 - val_loss: 1.4707\n","Epoch 5/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4606 - loss: 1.4163 - val_accuracy: 0.4690 - val_loss: 1.3478\n","Epoch 6/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4840 - loss: 1.3052 - val_accuracy: 0.4990 - val_loss: 1.2572\n","Epoch 7/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5178 - loss: 1.2038 - val_accuracy: 0.5530 - val_loss: 1.1489\n","Epoch 8/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5949 - loss: 1.0803 - val_accuracy: 0.6630 - val_loss: 0.9565\n","Epoch 9/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6900 - loss: 0.8928 - val_accuracy: 0.6970 - val_loss: 0.7817\n","Epoch 10/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7226 - loss: 0.7357 - val_accuracy: 0.7270 - val_loss: 0.7002\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7253 - loss: 0.6974 \n","Test Loss: 0.695371687412262, Test Accuracy: 0.7300000190734863\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n","Input Sequence: [5, 8, 8, 4, 6]\n","True Output Sequence: [5, 8, 8, 0, 0]\n","Predicted Output Sequence: [8, 8, 8, 0, 0]\n"]}]}]}