{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQirFLFe0vA8j2hqYyW7AN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","## Recurrent Neural Networks\n","\n","### Basics of RNN\n","\n","RNN works well with sequence or range of inputs with respect to time.\n","Thus, it finds its wide application in NLP\n","Formal Definition\n","\n","A recurrent neural network is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence.\n","\n","## Detailed explanation with a sample usecase\n","\n","Let's consider a NLP usecase, (ie) classification of email as (spam/ham).\n","The title or the subject of the email is considered as the dataset.\n","Let us consider a sentence X = \"you have an offer\". The sentence X can be represented as word vectors using any of the previously discussed approaches such as Bag Of Words(BOW), Word2Vec, TF-IDF.\n","Vectorized representation be X = <X1, X2, X3, X4>\n","Forward propagation over time\n","\n","The input for a hidden layer for a recurrent neural network at a time instant t1 will be the vector for word X1 along with the weights W\n","An output o1 will be generated from the first layer.\n","The key factor that distinguishes RNN from traditional ANN is that the input to the next hidden layer will be the word vector X2 along with o1 summed up together with the weights\n","Thus, from this it is evident that the sequence or order is preserved as the next hidden layer input depends on the previous layer at a time instant t.\n","Let us consider the below image\n","picture\n","\n","For a time instant t1 or t+1, the next word vector is considered and o2 is obtained as a function of o1+w and corresponding input X2\n","Multiple hidden layers can be created based on this weight forwarding technique (weight recurrence)\n","Thus, finally a softmax loss function can be used to classify (0 and 1) as spam and ham\n","Backward propagation over time\n","\n","The main reason to take up backpropagation is to reduce the loss\n","To reduce the loss, we indeed have to update the weights\n","Weight updation can be done by simply taking the derivative for the original weights (using chain rule maybe)\n","Subtract the original weights by the derivative value and update the value\n","Once when the global minima is reached (zero), RNN training will stop\n","Problems with RNN\n","In case of RNN, everytime the weights get updated based on the previous input in backpropagation\n","\n","Main issue - Vanishing gradient - In case of sigmoid function, when the derivative is found for weights, it becomes so very less that it is negligible and makes no difference in the next hidden layer's weight - does not converge\n","\n","When we use any other activation function like ReLu, exploding gradient problem occurs\n","\n","To overcome this issue, LSTM with RNNs are used\n","\n","LSTM Recurrent Neural Network\n","Link : https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n","Consider a usecase of text generation\n","\n","Say the model has generated the sentence \"My firstname is Hema\"\n","In this case , to generate a similar sentence and a slight difference the model has to understand that there is a change in context.\n","Eg: \"My surname is Priya\" . Here, the name has changed and the model needs to forget my old data and remember the new information\n","This is where LSTM with RNN comes into picture\n","LSTM generally consists of four important components\n","\n","Input gate\n","Memory cell\n","Forget gate\n","Output gate\n","Memory cell\n","\n","It temporarily remembers and forgets the vectors . (ie) when the original vector is say [1, 1, 2, 1, 3], the next time when the vector is updated the vector goes like [1, 1, 0, 0, 1, 3] . Thus, information related to 3rd and 4th word is forgotten by the hidden layer\n","\n","Forget Gate\n","\n","When the context is changed , the ouput vector will be changed and the previous vector will be removed or forgetted. Thus, the state of the vector is lost.\n","\n","Input Gate\n","\n","Y = WX + B where X acts as the input to the hidden layer along with the added information to the memory cell\n","\n","Output Gate\n","\n","All information in the memory cell is carried over back to the output layer finally (ie) memory cell + weights"],"metadata":{"id":"O39mgdwvXG6-"}},{"cell_type":"markdown","source":["# Word Embeddings\n","Word Embeddings\n","There are already a number of methods to convert word to numerical values or vectors (ie) BOW (Bag of words), TF-IDF\n","But, these methods have many disadvantages such as lack of semantic information\n","A method called One hot encoding was introduced\n","One hot encoding\n","\n","consider a sentence s1 - \"I like eating apples\".\n","sentence s2 - \"I like eating mangoes\"\n","The corresponding one-hot vector can be represented as [1, 1, 1, 1] for both the sentences\n","Assuming to determine the simiarity or closeness for the sentences, the result would be [1, 1, 1, 0] as only the last word varies(based on index).\n","In this case, for many sentences or huge corpus, such similar vectors might be obtained so the semantic is lost (ie difference between the word apples and mangoes is not justified).\n","Thus, the word embeddings come into picture\n","word embeddings\n","\n","It can also be described as feature based representation\n","In a huge corpus or dataset, say 10,000 sentences , a sample of 300 features can be considered and vectors of dimension 300 can be created.\n","Else, a vector will have dimension of 10,000 in case of one-hot encoding\n","For the previous example, under the feature or category of fruit, the word apples and mangoes can be categorized so the individual vector value may vary , thus preserving the semantic\n","Word embeddings provide a dense representation of words and their relative meanings.They are an improvement over sparse representations used in simpler bag of word model representations.Word embeddings can be learned from text data and reused among projects. They can also be learned as part of fitting a neural network on text data.\n","\n","**Implementation of word embeddings**\n","\n","Keras , by default has embeddings layer . But initially the words are tokenized, and one-hot encoding is applied, and the dimensions or number of features to be considered is defined to obtain the word embeddings"],"metadata":{"id":"XYF1r5pJXZ1t"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"10CmaT5cW6d8","executionInfo":{"status":"ok","timestamp":1726666955144,"user_tz":-330,"elapsed":9362,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"905015a6-5a34-43e9-c7c8-73da8a6390fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.17.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["import tensorflow as tf\n","tf.__version__"]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import one_hot"],"metadata":{"id":"1U-4R2eKYCrp","executionInfo":{"status":"ok","timestamp":1726667047770,"user_tz":-330,"elapsed":441,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["sentences = [\n","             \"I am a good girl\",\n","             \"I am an engineer\",\n","             \"The sum rises in the east\",\n","             \"Live life to the fullest\",\n","             \"I am a developer\",\n","             \"I need a cup of tea\",\n","             \"I can understand\",\n","             \"my work is good\",\n","             \"I like apples\",\n","             \"I don't like mangoes\"\n","]"],"metadata":{"id":"sGY_xQPyYEXU","executionInfo":{"status":"ok","timestamp":1726667070487,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#define the vocabulary size\n","vocab_size = 1000\n"],"metadata":{"id":"IIaEdC4FYJwb","executionInfo":{"status":"ok","timestamp":1726667083719,"user_tz":-330,"elapsed":1,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["onehot_repr=[one_hot(words,vocab_size)for words in sentences]\n","print(onehot_repr)\n","\n","#index based on the created vocabulary/dictionary will be obtained\n","#length of list and length of every sentence is same\n","# 159 => I 39 => am"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UJ-e4vTwYNOe","executionInfo":{"status":"ok","timestamp":1726667107755,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"0f9472b9-a68e-4c55-a622-7286858d8ebb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[567, 730, 999, 659, 755], [567, 730, 579, 850], [223, 461, 16, 44, 223, 874], [982, 799, 587, 223, 608], [567, 730, 999, 659], [567, 277, 999, 892, 698, 819], [567, 910, 370], [416, 661, 508, 659], [567, 261, 79], [567, 593, 261, 889]]\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences #to make length of sentence equal\n","from tensorflow.keras.models import Sequential\n","import numpy as np"],"metadata":{"id":"n7Mewn5GYNIk","executionInfo":{"status":"ok","timestamp":1726667119770,"user_tz":-330,"elapsed":480,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","#Embedding representation\n","\n","sent_length=8\n","embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n","print(embedded_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DL3BMrxNYVRV","executionInfo":{"status":"ok","timestamp":1726667127931,"user_tz":-330,"elapsed":476,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"2b76da7d-1de1-431f-cfcd-e8c8e7904fb6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  0   0   0 567 730 999 659 755]\n"," [  0   0   0   0 567 730 579 850]\n"," [  0   0 223 461  16  44 223 874]\n"," [  0   0   0 982 799 587 223 608]\n"," [  0   0   0   0 567 730 999 659]\n"," [  0   0 567 277 999 892 698 819]\n"," [  0   0   0   0   0 567 910 370]\n"," [  0   0   0   0 416 661 508 659]\n"," [  0   0   0   0   0 567 261  79]\n"," [  0   0   0   0 567 593 261 889]]\n"]}]},{"cell_type":"code","source":["#define the number of features\n","\n","dim=10\n","\n","#define the model\n","model=Sequential()\n","model.add(Embedding(vocab_size,10,input_length=sent_length))\n","model.compile('adam','mse')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AsYne7AYXvr","executionInfo":{"status":"ok","timestamp":1726667161917,"user_tz":-330,"elapsed":460,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"af7ceaa2-4d6d-4329-c908-605a2ae804f7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["print(model.predict(embedded_docs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kM2B_MyYgYF","executionInfo":{"status":"ok","timestamp":1726667173222,"user_tz":-330,"elapsed":503,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"a1f6a69a-4525-4bbd-df62-bf1f80dcbaa4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n","[[[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 2.66976282e-03 -4.73067537e-02  4.03023884e-03 -2.41728425e-02\n","   -9.38636065e-03 -3.15554291e-02 -1.87604185e-02 -4.96016257e-02\n","   -3.23021039e-02  3.21363322e-02]\n","  [ 1.96955241e-02 -3.91077250e-04  9.55078751e-03 -2.01512501e-03\n","   -8.70171934e-03  2.33808421e-02  2.26479657e-02  2.14028694e-02\n","    1.53617971e-02  6.81634992e-03]\n","  [-1.83971412e-02 -2.30427142e-02  1.92599036e-02  2.41428502e-02\n","    2.27301382e-02  5.49482182e-03  2.96469815e-02 -2.44205240e-02\n","    2.24722065e-02  1.86494254e-02]\n","  [ 3.02006342e-02 -4.92037199e-02 -4.77853306e-02  4.04649265e-02\n","   -3.64012122e-02  4.12991531e-02 -5.94747066e-03 -3.48590501e-02\n","    2.24581696e-02  1.99797004e-03]\n","  [ 1.11207142e-02 -1.30996816e-02 -2.29195505e-03  1.27422325e-02\n","   -4.88759652e-02  3.45004909e-02  4.99880798e-02  4.00947817e-02\n","    1.74819939e-02 -2.69382726e-02]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 2.66976282e-03 -4.73067537e-02  4.03023884e-03 -2.41728425e-02\n","   -9.38636065e-03 -3.15554291e-02 -1.87604185e-02 -4.96016257e-02\n","   -3.23021039e-02  3.21363322e-02]\n","  [ 1.96955241e-02 -3.91077250e-04  9.55078751e-03 -2.01512501e-03\n","   -8.70171934e-03  2.33808421e-02  2.26479657e-02  2.14028694e-02\n","    1.53617971e-02  6.81634992e-03]\n","  [-2.74038911e-02 -1.44200437e-02 -3.97425517e-02  5.40369749e-03\n","   -6.35059923e-03 -6.78078085e-03  1.50812156e-02 -2.21553091e-02\n","   -2.14331746e-02 -4.26026359e-02]\n","  [ 4.59210612e-02 -4.33717631e-02  1.80155896e-02  3.53209861e-02\n","    2.67717727e-02  1.72419213e-02 -1.52351484e-02  2.91124023e-02\n","    1.00587681e-03 -3.11186444e-02]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.13999073e-02  1.56674869e-02  1.37662329e-02 -4.35044765e-02\n","   -1.16395466e-02  4.13012616e-02 -4.95090149e-02 -1.80678591e-02\n","   -6.54187053e-03  3.21895368e-02]\n","  [ 4.13334705e-02 -2.94491183e-02 -4.14892063e-02 -3.83010507e-02\n","   -2.14624535e-02 -2.34277174e-03  2.94782966e-03 -2.40860339e-02\n","   -4.93936539e-02 -3.17807570e-02]\n","  [ 1.54286735e-02 -3.80567200e-02 -5.22829592e-04 -3.61458883e-02\n","    3.80929746e-02 -1.34707466e-02  4.61130850e-02 -3.35148349e-02\n","    3.92600633e-02 -3.76998559e-02]\n","  [-4.63525541e-02 -3.83944400e-02 -3.24525982e-02 -3.14677954e-02\n","    4.82831709e-02  3.22155990e-02  4.45292927e-02  4.94239368e-02\n","    2.48773023e-03  3.15533765e-02]\n","  [ 4.13999073e-02  1.56674869e-02  1.37662329e-02 -4.35044765e-02\n","   -1.16395466e-02  4.13012616e-02 -4.95090149e-02 -1.80678591e-02\n","   -6.54187053e-03  3.21895368e-02]\n","  [-6.86596707e-03 -4.43612337e-02  2.51534097e-02 -4.21497598e-02\n","   -3.85636911e-02 -2.50457413e-02 -1.70171857e-02  2.58783586e-02\n","   -3.18829641e-02  2.61198543e-02]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [-4.90734950e-02  2.63281204e-02 -3.54836099e-02  1.72015280e-03\n","    1.56383403e-02  2.52364017e-02 -1.45291090e-02 -1.17776394e-02\n","    1.57910921e-02 -8.69495794e-03]\n","  [ 1.40211619e-02  1.07197277e-02  2.41243839e-03 -4.30949107e-02\n","   -3.10536623e-02 -3.21571007e-02 -1.49540082e-02  1.56195089e-03\n","    1.51372291e-02  3.39184664e-02]\n","  [ 2.98051499e-02  1.50228851e-02  4.53396179e-02  3.16552781e-02\n","    5.44837862e-03  3.91888283e-02 -3.11630499e-02 -1.36663429e-02\n","   -1.52739882e-02 -1.22312084e-02]\n","  [ 4.13999073e-02  1.56674869e-02  1.37662329e-02 -4.35044765e-02\n","   -1.16395466e-02  4.13012616e-02 -4.95090149e-02 -1.80678591e-02\n","   -6.54187053e-03  3.21895368e-02]\n","  [ 4.16710228e-03  1.48137845e-02 -9.11885500e-03 -2.60192398e-02\n","    4.76180799e-02  4.00400646e-02  1.03661045e-02 -1.75709836e-02\n","   -4.74698208e-02 -2.10541487e-03]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 2.66976282e-03 -4.73067537e-02  4.03023884e-03 -2.41728425e-02\n","   -9.38636065e-03 -3.15554291e-02 -1.87604185e-02 -4.96016257e-02\n","   -3.23021039e-02  3.21363322e-02]\n","  [ 1.96955241e-02 -3.91077250e-04  9.55078751e-03 -2.01512501e-03\n","   -8.70171934e-03  2.33808421e-02  2.26479657e-02  2.14028694e-02\n","    1.53617971e-02  6.81634992e-03]\n","  [-1.83971412e-02 -2.30427142e-02  1.92599036e-02  2.41428502e-02\n","    2.27301382e-02  5.49482182e-03  2.96469815e-02 -2.44205240e-02\n","    2.24722065e-02  1.86494254e-02]\n","  [ 3.02006342e-02 -4.92037199e-02 -4.77853306e-02  4.04649265e-02\n","   -3.64012122e-02  4.12991531e-02 -5.94747066e-03 -3.48590501e-02\n","    2.24581696e-02  1.99797004e-03]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 2.66976282e-03 -4.73067537e-02  4.03023884e-03 -2.41728425e-02\n","   -9.38636065e-03 -3.15554291e-02 -1.87604185e-02 -4.96016257e-02\n","   -3.23021039e-02  3.21363322e-02]\n","  [ 1.41879357e-02  9.11534950e-03  3.34854387e-02 -9.65193659e-03\n","   -3.70880477e-02  1.17399916e-02 -2.23805904e-02 -4.50441614e-02\n","    2.40253471e-02 -1.28127225e-02]\n","  [-1.83971412e-02 -2.30427142e-02  1.92599036e-02  2.41428502e-02\n","    2.27301382e-02  5.49482182e-03  2.96469815e-02 -2.44205240e-02\n","    2.24722065e-02  1.86494254e-02]\n","  [ 3.67355235e-02 -5.87994978e-03 -3.90927419e-02  7.41934776e-03\n","   -9.01484489e-03  6.85840845e-03  3.42058875e-02  4.94921915e-02\n","   -1.39836222e-03 -1.24141574e-02]\n","  [ 4.71109636e-02  5.34834713e-03 -2.64199860e-02  1.22114792e-02\n","    4.76383232e-02  3.21599878e-02 -2.32459195e-02  2.56890319e-02\n","   -3.22280899e-02  3.16860341e-02]\n","  [ 7.73171335e-03 -7.70032406e-04 -2.43178960e-02  2.13975199e-02\n","   -1.78352371e-02 -2.70512830e-02  1.22922771e-02 -3.58311087e-03\n","    2.95991190e-02 -3.94900814e-02]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 2.66976282e-03 -4.73067537e-02  4.03023884e-03 -2.41728425e-02\n","   -9.38636065e-03 -3.15554291e-02 -1.87604185e-02 -4.96016257e-02\n","   -3.23021039e-02  3.21363322e-02]\n","  [-8.89524817e-05  1.99615471e-02  2.02151425e-02 -1.49310343e-02\n","   -1.61854178e-03  1.08987093e-03  1.40468217e-02 -4.22687307e-02\n","   -1.24860629e-02  6.20820373e-03]\n","  [-3.66616994e-04  1.50788166e-02  1.74729899e-03 -1.03367567e-02\n","    1.09678879e-02  2.94744708e-02 -3.65673080e-02  2.75906585e-02\n","   -2.72986777e-02 -4.29116264e-02]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 3.98269780e-02 -4.77912426e-02 -4.72701304e-02  4.79837991e-02\n","    2.11112164e-02 -2.83688307e-03  2.86568515e-02 -2.40607262e-02\n","    2.17662491e-02  4.35579307e-02]\n","  [-7.59152323e-03  3.40294875e-02 -9.47042555e-03 -2.12659594e-02\n","   -1.47337094e-02 -3.70092504e-02  1.90194361e-02  2.06505395e-02\n","    3.53191160e-02 -1.95571426e-02]\n","  [ 2.83121578e-02 -4.00635824e-02  4.71908338e-02  1.29537024e-02\n","    3.44916433e-03  4.37451862e-02 -2.80280709e-02  9.62888077e-03\n","   -3.33597884e-02 -4.17516381e-03]\n","  [ 3.02006342e-02 -4.92037199e-02 -4.77853306e-02  4.04649265e-02\n","   -3.64012122e-02  4.12991531e-02 -5.94747066e-03 -3.48590501e-02\n","    2.24581696e-02  1.99797004e-03]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 2.66976282e-03 -4.73067537e-02  4.03023884e-03 -2.41728425e-02\n","   -9.38636065e-03 -3.15554291e-02 -1.87604185e-02 -4.96016257e-02\n","   -3.23021039e-02  3.21363322e-02]\n","  [-4.84899431e-03  3.41009982e-02  4.14255522e-02  1.77784003e-02\n","    3.86491753e-02 -2.43658070e-02  2.57331990e-02  2.38502286e-02\n","   -1.41429417e-02  1.47222392e-02]\n","  [-2.27613933e-02 -4.03923504e-02 -9.77915525e-03 -2.37968210e-02\n","    1.80595256e-02  4.83549349e-02  2.63661034e-02  7.38958269e-03\n","    3.46831344e-02 -3.96672972e-02]]\n","\n"," [[ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 4.87896465e-02 -1.56032331e-02  4.89363559e-02 -3.27865630e-02\n","    2.46693380e-02  2.61197127e-02  4.76915762e-03 -6.66989014e-03\n","    2.38736011e-02  3.01277302e-02]\n","  [ 2.66976282e-03 -4.73067537e-02  4.03023884e-03 -2.41728425e-02\n","   -9.38636065e-03 -3.15554291e-02 -1.87604185e-02 -4.96016257e-02\n","   -3.23021039e-02  3.21363322e-02]\n","  [-2.75309570e-02  4.75631841e-02 -3.78390662e-02 -1.59522146e-03\n","    1.60485394e-02  4.92195971e-02  4.76817824e-02 -6.80448860e-03\n","   -2.15590131e-02 -4.54336889e-02]\n","  [-4.84899431e-03  3.41009982e-02  4.14255522e-02  1.77784003e-02\n","    3.86491753e-02 -2.43658070e-02  2.57331990e-02  2.38502286e-02\n","   -1.41429417e-02  1.47222392e-02]\n","  [ 2.22269446e-03  3.97423841e-02 -1.66396387e-02 -4.78460565e-02\n","   -2.05352902e-02  7.58908689e-04 -4.08535600e-02 -2.86375880e-02\n","   -1.59186944e-02  2.94040106e-02]]]\n"]}]},{"cell_type":"code","source":["print(model.predict(embedded_docs)[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWTJIelIYkm0","executionInfo":{"status":"ok","timestamp":1726667182734,"user_tz":-330,"elapsed":464,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"470da23d-b5ff-405a-ccb1-3c695f4fddb0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","[[ 0.04878965 -0.01560323  0.04893636 -0.03278656  0.02466934  0.02611971\n","   0.00476916 -0.00666989  0.0238736   0.03012773]\n"," [ 0.04878965 -0.01560323  0.04893636 -0.03278656  0.02466934  0.02611971\n","   0.00476916 -0.00666989  0.0238736   0.03012773]\n"," [ 0.04878965 -0.01560323  0.04893636 -0.03278656  0.02466934  0.02611971\n","   0.00476916 -0.00666989  0.0238736   0.03012773]\n"," [ 0.00266976 -0.04730675  0.00403024 -0.02417284 -0.00938636 -0.03155543\n","  -0.01876042 -0.04960163 -0.0323021   0.03213633]\n"," [ 0.01969552 -0.00039108  0.00955079 -0.00201513 -0.00870172  0.02338084\n","   0.02264797  0.02140287  0.0153618   0.00681635]\n"," [-0.01839714 -0.02304271  0.0192599   0.02414285  0.02273014  0.00549482\n","   0.02964698 -0.02442052  0.02247221  0.01864943]\n"," [ 0.03020063 -0.04920372 -0.04778533  0.04046493 -0.03640121  0.04129915\n","  -0.00594747 -0.03485905  0.02245817  0.00199797]\n"," [ 0.01112071 -0.01309968 -0.00229196  0.01274223 -0.04887597  0.03450049\n","   0.04998808  0.04009478  0.01748199 -0.02693827]]\n"]}]}]}