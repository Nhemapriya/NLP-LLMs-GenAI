{"cells":[{"cell_type":"markdown","metadata":{"id":"yYtA-wTZNrjw"},"source":["# Neural Machine Learning Language Translation using encoder-decoder\n","\n","Dataset : French to english translation\n","\n","http://www.manythings.org/anki/\n","\n","Reference : https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"]},{"cell_type":"markdown","metadata":{"id":"0dtQoYZyuTfR"},"source":["- We start with input sequences from a domain (e.g. English sentences)\n","    and corresponding target sequences from another domain\n","    (e.g. French sentences).\n","- An encoder LSTM turns input sequences to 2 state vectors\n","    (we keep the last LSTM state and discard the outputs).\n","- A decoder LSTM is trained to turn the target sequences into\n","    the same sequence but offset by one timestep in the future,\n","    a training process called \"teacher forcing\" in this context.\n","    It uses as initial state the state vectors from the encoder.\n","    Effectively, the decoder learns to generate `targets[t+1...]`\n","    given `targets[...t]`, conditioned on the input sequence.\n","- In inference mode, when we want to decode unknown input sequences, we:\n","    - Encode the input sequence into state vectors\n","    - Start with a target sequence of size 1\n","        (just the start-of-sequence character)\n","    - Feed the state vectors and 1-char target sequence\n","        to the decoder to produce predictions for the next character\n","    - Sample the next character using these predictions\n","        (we simply use argmax).\n","    - Append the sampled character to the target sequence\n","    - Repeat until we generate the end-of-sequence character or we\n","        hit the character limit."]},{"cell_type":"markdown","metadata":{"id":"gUiBvYiXOMQt"},"source":["**Initialization and imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24JT1JesNhdI"},"outputs":[],"source":["import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, Dense, LSTM\n","\n","batch_size = 64\n","epochs = 10\n","#dimensions of latent vector\n","latent_dim = 256\n","#total number of samples\n","num_samples = 10000\n","#path to dataset\n","data_path = '/content/drive/MyDrive/NLP/fra.txt'"]},{"cell_type":"markdown","metadata":{"id":"nF0C0w3AO9KA"},"source":["**vectorize the data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJgoZ20OOL0-"},"outputs":[],"source":["#english and french words\n","input_texts = []\n","target_texts = []\n","\n","#english and french characters\n","input_characters = set()\n","target_characters = set()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5r49utZPBMW"},"outputs":[],"source":["\n","#open file in read mode\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")\n","\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, _ = line.split(\"\\t\")\n","\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = \"\\t\" + target_text + \"\\n\"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","\n","    #add the leftout characters\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45169,"status":"ok","timestamp":1726675772766,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"},"user_tz":-330},"id":"phw8n5PW47q2","outputId":"1de083ba-5760-40ed-9788-3834d2b98d9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"nIxWA6pNY1fw"},"source":["**Define the parameters in variables**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKvSHpG1V0V4"},"outputs":[],"source":["input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pakGaaeOLx7"},"outputs":[],"source":["#obtain the dictionary tokens and pair\n","\n","input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1726675893748,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"},"user_tz":-330},"id":"gp4BWhS0bPn6","outputId":"7beff18f-7e17-43f9-93a8-60a8c0c4bb9d"},"outputs":[{"data":{"text/plain":["{' ': 0,\n"," '!': 1,\n"," '\"': 2,\n"," '$': 3,\n"," '%': 4,\n"," '&': 5,\n"," \"'\": 6,\n"," ',': 7,\n"," '-': 8,\n"," '.': 9,\n"," '0': 10,\n"," '1': 11,\n"," '2': 12,\n"," '3': 13,\n"," '5': 14,\n"," '7': 15,\n"," '8': 16,\n"," '9': 17,\n"," ':': 18,\n"," '?': 19,\n"," 'A': 20,\n"," 'B': 21,\n"," 'C': 22,\n"," 'D': 23,\n"," 'E': 24,\n"," 'F': 25,\n"," 'G': 26,\n"," 'H': 27,\n"," 'I': 28,\n"," 'J': 29,\n"," 'K': 30,\n"," 'L': 31,\n"," 'M': 32,\n"," 'N': 33,\n"," 'O': 34,\n"," 'P': 35,\n"," 'Q': 36,\n"," 'R': 37,\n"," 'S': 38,\n"," 'T': 39,\n"," 'U': 40,\n"," 'V': 41,\n"," 'W': 42,\n"," 'Y': 43,\n"," 'a': 44,\n"," 'b': 45,\n"," 'c': 46,\n"," 'd': 47,\n"," 'e': 48,\n"," 'f': 49,\n"," 'g': 50,\n"," 'h': 51,\n"," 'i': 52,\n"," 'j': 53,\n"," 'k': 54,\n"," 'l': 55,\n"," 'm': 56,\n"," 'n': 57,\n"," 'o': 58,\n"," 'p': 59,\n"," 'q': 60,\n"," 'r': 61,\n"," 's': 62,\n"," 't': 63,\n"," 'u': 64,\n"," 'v': 65,\n"," 'w': 66,\n"," 'x': 67,\n"," 'y': 68,\n"," 'z': 69,\n"," 'é': 70}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["input_token_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcLjZTO5a34u"},"outputs":[],"source":["#defining the french data parameters\n","\n","#create a numpy array for encoder i/p and decoder i/p & o/p\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",")\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wrbzx500r-Op"},"source":["**one hot representation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3j56E6Mr7NA"},"outputs":[],"source":["for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.0\n","    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":458,"status":"ok","timestamp":1726675902704,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"},"user_tz":-330},"id":"0kATVhsPbvz6","outputId":"fba2f366-01cf-4991-f3b8-d54789a1692b"},"outputs":[{"data":{"text/plain":["(15, 71)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["encoder_input_data[0].shape"]},{"cell_type":"markdown","metadata":{"id":"F1wkRb4ju0FC"},"source":["**Model construction**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCap5FT1bwTC"},"outputs":[],"source":["# Define an input sequence and process it.\n","\n","from tensorflow import keras\n","encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n","encoder = keras.layers.LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n","\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"markdown","metadata":{"id":"v1D6YDwivHAw"},"source":["**Train the model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11jDgSKxvEaZ"},"outputs":[],"source":["model.compile(\n","    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"dqGWOLAlvK-F","outputId":"b60123f7-9018-40b2-f612-afb5860c8db7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 343ms/step - accuracy: 0.7042 - loss: 1.5541 - val_accuracy: 0.7065 - val_loss: 1.0975\n","Epoch 2/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 335ms/step - accuracy: 0.7442 - loss: 0.9639 - val_accuracy: 0.7149 - val_loss: 1.0025\n","Epoch 3/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 353ms/step - accuracy: 0.7594 - loss: 0.8756 - val_accuracy: 0.7308 - val_loss: 0.9964\n","Epoch 4/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 339ms/step - accuracy: 0.7852 - loss: 0.7814 - val_accuracy: 0.7663 - val_loss: 0.8173\n","Epoch 5/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 338ms/step - accuracy: 0.8017 - loss: 0.6976 - val_accuracy: 0.7735 - val_loss: 0.7840\n","Epoch 6/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 337ms/step - accuracy: 0.8120 - loss: 0.6495 - val_accuracy: 0.7899 - val_loss: 0.7243\n","Epoch 7/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 340ms/step - accuracy: 0.8214 - loss: 0.6131 - val_accuracy: 0.7904 - val_loss: 0.7025\n","Epoch 8/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 336ms/step - accuracy: 0.8260 - loss: 0.5939 - val_accuracy: 0.8032 - val_loss: 0.6805\n","Epoch 9/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 340ms/step - accuracy: 0.8309 - loss: 0.5736 - val_accuracy: 0.8057 - val_loss: 0.6540\n","Epoch 10/10\n","\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 337ms/step - accuracy: 0.8367 - loss: 0.5521 - val_accuracy: 0.8129 - val_loss: 0.6369\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7cdb2c57a380>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_split=0.2,\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_A10O3D4mcT","outputId":"72d560c9-db27-45e1-e22d-2e60fbf0164c","executionInfo":{"status":"ok","timestamp":1726679251836,"user_tz":-330,"elapsed":451,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}],"source":["# Save model\n","model.save(\"s2s.h5\")"]},{"cell_type":"markdown","metadata":{"id":"cx7tTfcB46Q2"},"source":["**check the model**"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"TWpkOYBY4oK6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726679259351,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}},"outputId":"0033b4be-8a01-42b3-e094-c8c235278846"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}],"source":["\"\"\"\n","## Run inference (sampling)\n","1. encode input and retrieve initial decoder state\n","2. run one step of decoder with this initial state\n","and a \"start of sequence\" token as target.\n","Output will be the next target token.\n","3. Repeat with the current target token and current states\n","\"\"\"\n","\n","# Define sampling models\n","# Restore the model and construct the encoder and decoder.\n","model = keras.models.load_model(\"s2s.h5\")\n","\n","encoder_inputs = model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","decoder_inputs = model.input[1]  # input_2\n","decoder_state_input_h = keras.Input(shape=(latent_dim,))\n","decoder_state_input_c = keras.Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_lstm = model.layers[3]\n","decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h_dec, state_c_dec]\n","decoder_dense = model.layers[4]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = keras.Model(\n","    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",")\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ri5qsEmi4y2p","executionInfo":{"status":"ok","timestamp":1726679262303,"user_tz":-330,"elapsed":478,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"outputs":[],"source":["def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    return decoded_sentence"]},{"cell_type":"markdown","metadata":{"id":"zdjosGCt4tqP"},"source":["**sentence generation**"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQP89r8C4s05","outputId":"4c57a6cc-c56e-437f-e8d6-5b2c0f1e124a","executionInfo":{"status":"ok","timestamp":1726679274442,"user_tz":-330,"elapsed":10361,"user":{"displayName":"Hema Priya","userId":"09549907028764281568"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","-\n","Input sentence: Go.\n","Decoded sentence: Sontez le cous !\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","-\n","Input sentence: Go.\n","Decoded sentence: Sontez le cous !\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","-\n","Input sentence: Go.\n","Decoded sentence: Sontez le cous !\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","-\n","Input sentence: Hi.\n","Decoded sentence: Nous sommes paste.\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","-\n","Input sentence: Hi.\n","Decoded sentence: Nous sommes paste.\n","\n"]}],"source":["for seq_index in range(5):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index : seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print(\"-\")\n","    print(\"Input sentence:\", input_texts[seq_index])\n","    print(\"Decoded sentence:\", decoded_sentence)"]},{"cell_type":"markdown","metadata":{"id":"L86DakQqzz2f"},"source":["# Problems with encoder and decoder\n","\n","\n","\n","1.   We know that the ouput of the encoder is the context vector and that is given as input to the decoder. The encoder/decoder architecture does not work well for **longer sentences**\n","2.   In case of longer sentences, the actual information regarding the sentences are not captured completely by the encoder and thus, when given as a input to the decoder, it does not reproduce the outputs effectively.\n","3.   It is observed that if the sentence increases in length , the accuracy decreases.\n","4.   To overcome this problem, **attention models are used** where bidirectional LSTMs are used in encoders to capture all information\n","5.   Attention models make use of window size . (ie) when a paragraph of 1000 words is given , at an instant a window of 10 words is considered and trained incrementally\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}