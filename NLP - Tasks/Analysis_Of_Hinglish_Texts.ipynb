{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Use of Luong attention based encoder-decoder architecture model to perform the analysis of Hinglish text"],"metadata":{"id":"4e-qWJeZ3qaL"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_MoTR8qYn_0h","colab":{"base_uri":"https://localhost:8080/"},"outputId":"083c7cfe-c878-41bb-b042-a21313226d40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NALAxker3ZBi","outputId":"e59bfde5-8701-4cfe-eaef-7fa5072b50df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Dec  9 19:21:00 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   67C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","source":["**Import the required dependencies**"],"metadata":{"id":"mOLu3qfr4r_r"}},{"cell_type":"code","source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","from string import digits\n","import pandas as pd"],"metadata":{"id":"chcc-vrP4ihD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import gc\n","import time\n","import re\n","import unicodedata\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","#Importing libraries\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"W7YUamrD4la-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(w):\n","    ''' Preprocess the input text w applying lowercase, removing accents,\n","    creating a space between a word and the punctuation following it and\n","    replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    Input:\n","        - w: a string, input text\n","    Output:\n","        - a string, the cleaned text\n","    '''\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","\n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","    w = w.strip()\n","\n","    return w"],"metadata":{"id":"R2oPguD547Mj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the dataset: sentence in english\n","\n","df=pd.read_excel(\"/content/drive/MyDrive/NLP/Hinglish50k.xlsx\")\n","\n","# Preprocess the input data\n","input_data=df[\"hindi_sentences\"].apply(lambda x : preprocess_sentence(x)).tolist()\n","\n","# Preprocess and include the end of sentence token to the target text\n","target_data=df[\"english_sentences\"].apply(lambda x : preprocess_sentence(x)+ ' <eos>').tolist()\n","\n","# Preprocess and include a start of setence token to the input text to the decoder, it is rigth shifted\n","target_input_data=df[\"english_sentences\"].apply(lambda x : '<sos> '+ preprocess_sentence(x)).tolist()\n","\n","print(input_data[:5])\n","print(target_data[:5])\n","print(target_input_data[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVcM6Q4N5ENw","outputId":"b94366d6-c82c-44f8-bece-86fbb7ee8ff0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['aur', 'eh .', 'men', 'oh .', 'aikya']\n","['and <eos>', 'eh . <eos>', 'men <eos>', 'oh . <eos>', 'iq <eos>']\n","['<sos> and', '<sos> eh .', '<sos> men', '<sos> oh .', '<sos> iq']\n"]}]},{"cell_type":"code","source":["MAX_VOCAB_SIZE = 20000\n","BATCH_SIZE = 32  # Batch size for training.\n","EPOCHS = 15  # Number of epochs"],"metadata":{"id":"nj6WKtfN5aNL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a tokenizer for the input texts and fit it to them\n","tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n","tokenizer_inputs.fit_on_texts(input_data)\n","# Tokenize and transform input texts to sequence of integers\n","input_sequences = tokenizer_inputs.texts_to_sequences(input_data)\n","# Claculate the max length\n","input_max_len = max(len(s) for s in input_sequences)\n","print('Max Input Length: ', input_max_len)\n","# Show some example of tokenize sentences, useful to check the tokenization\n","print(input_data[1000])\n","print(input_sequences[1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4hzv2fp5dhH","outputId":"5c81cf10-25c2-48c6-a645-f2e110cacf04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max Input Length:  24\n","yah dekho .\n","[17, 843, 1]\n"]}]},{"cell_type":"code","source":["tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n","tokenizer_outputs.fit_on_texts(target_data)\n","tokenizer_outputs.fit_on_texts(target_input_data)\n","# T0okenize and transform output texts to sequence of integers\n","target_sequences = tokenizer_outputs.texts_to_sequences(target_data)\n","target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_input_data)\n","\n","# determine maximum length output sequence\n","target_max_len = max(len(s) for s in target_sequences)\n","print('Max Target Length: ', target_max_len)\n","\n","print(target_data[1000])\n","print(target_sequences[1000])\n","print(target_input_data[1000])\n","print(target_sequences_inputs[100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jknKd5s5gXK","outputId":"31bbacee-8e7d-4aca-9b6b-6b224b0eae8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max Target Length:  20\n","watch this . <eos>\n","[735, 13, 1, 2]\n","<sos> watch this .\n","[3, 8089]\n"]}]},{"cell_type":"code","source":["# get the word to index mapping for input language\n","word2idx_inputs = tokenizer_inputs.word_index\n","print('Found %s unique input tokens.' % len(word2idx_inputs))\n","\n","# get the word to index mapping for output language\n","word2idx_outputs = tokenizer_outputs.word_index\n","print('Found %s unique output tokens.' % len(word2idx_outputs))\n","\n","# store number of output and input words for later\n","# remember to add 1 since indexing starts at 1\n","num_words_output = len(word2idx_outputs) + 1\n","num_words_inputs = len(word2idx_inputs) + 1\n","\n","# map indexes back into real words\n","# so we can view the results\n","idx2word_inputs = {v:k for k, v in word2idx_inputs.items()}\n","idx2word_outputs = {v:k for k, v in word2idx_outputs.items()}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-nM9H4tV5jH5","outputId":"b9f574e2-ad91-41b7-f82e-206867ed3fc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 24570 unique input tokens.\n","Found 21995 unique output tokens.\n"]}]},{"cell_type":"code","source":["# pad the input sequences\n","encoder_inputs = pad_sequences(input_sequences, maxlen=input_max_len, padding='post')\n","print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n","print(\"encoder_inputs[0]:\", encoder_inputs[0])\n","# pad the decoder input sequences\n","decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=target_max_len, padding='post')\n","print(\"decoder_inputs[0]:\", decoder_inputs[0])\n","print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n","# pad the target output sequences\n","decoder_targets = pad_sequences(target_sequences, maxlen=target_max_len, padding='post')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZ4ELkaQ5ntX","outputId":"63d22752-83ab-4a99-e458-1e7db1bb757f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["encoder_inputs.shape: (49999, 24)\n","encoder_inputs[0]: [6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","decoder_inputs[0]: [3 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","decoder_inputs.shape: (49999, 20)\n"]}]},{"cell_type":"code","source":["# Define a dataset\n","dataset = tf.data.Dataset.from_tensor_slices(\n","    (encoder_inputs, decoder_inputs, decoder_targets))\n","dataset = dataset.shuffle(len(input_data)).batch(\n","    BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"uDAZMSo45jGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(Encoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        # Define the embedding layer\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        # Define the RNN layer, LSTM\n","        self.lstm = tf.keras.layers.LSTM(\n","            hidden_dim, return_sequences=True, return_state=True)\n","\n","    def call(self, input_sequence, states):\n","        # Embed the input\n","        embed = self.embedding(input_sequence)\n","        # Call the LSTM unit\n","        output, state_h, state_c = self.lstm(embed, initial_state=states)\n","\n","        return output, state_h, state_c\n","\n","    def init_states(self, batch_size):\n","        # Return a all 0s initial states\n","        return (tf.zeros([batch_size, self.hidden_dim]),\n","                tf.zeros([batch_size, self.hidden_dim]))"],"metadata":{"id":"DFAOtZBv5uhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, attention_func):\n","        super(Decoder, self).__init__()\n","        self.attention = LuongAttention(hidden_dim, attention_func)\n","        self.hidden_dim = hidden_dim\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.LSTM(\n","            hidden_dim, return_sequences=True, return_state=True)\n","        self.wc = tf.keras.layers.Dense(hidden_dim, activation='tanh')\n","        self.ws = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, input_sequence, state, encoder_output):\n","        # Remember that the input to the decoder\n","        # is now a batch of one-word sequences,\n","        # which means that its shape is (batch_size, 1)\n","        embed = self.embedding(input_sequence)\n","\n","        # Therefore, the lstm_out has shape (batch_size, 1, hidden_dim)\n","        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n","\n","        # Use self.attention to compute the context and alignment vectors\n","        # context vector's shape: (batch_size, 1, hidden_dim)\n","        # alignment vector's shape: (batch_size, 1, source_length)\n","        context, alignment = self.attention(lstm_out, encoder_output)\n","\n","        # Combine the context vector and the LSTM output\n","        # Before combined, both have shape of (batch_size, 1, hidden_dim),\n","        # so let's squeeze the axis 1 first\n","        # After combined, it will have shape of (batch_size, 2 * hidden_dim)\n","        lstm_out = tf.concat(\n","            [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n","\n","        # lstm_out now has shape (batch_size, hidden_dim)\n","        lstm_out = self.wc(lstm_out)\n","\n","        # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n","        logits = self.ws(lstm_out)\n","\n","        return logits, state_h, state_c, alignment"],"metadata":{"id":"fBQYiqBD5_xf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LuongAttention(tf.keras.Model):\n","    def __init__(self, rnn_size, attention_func):\n","        super(LuongAttention, self).__init__()\n","        self.attention_func = attention_func\n","\n","        if attention_func not in ['dot', 'general', 'concat']:\n","            raise ValueError(\n","                'Attention score must be either dot, general or concat.')\n","\n","        if attention_func == 'general':\n","            # General score function\n","            self.wa = tf.keras.layers.Dense(rnn_size)\n","        elif attention_func == 'concat':\n","            # Concat score function\n","            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n","            self.va = tf.keras.layers.Dense(1)\n","\n","    def call(self, decoder_output, encoder_output):\n","        if self.attention_func == 'dot':\n","            # Dot score function: decoder_output (dot) encoder_output\n","            # decoder_output has shape: (batch_size, 1, rnn_size)\n","            # encoder_output has shape: (batch_size, max_len, rnn_size)\n","            # => score has shape: (batch_size, 1, max_len)\n","            score = tf.matmul(decoder_output, encoder_output, transpose_b=True) # (batch_size, 1, max_len)\n","        elif self.attention_func == 'general':\n","            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n","            # decoder_output has shape: (batch_size, 1, rnn_size)\n","            # encoder_output has shape: (batch_size, max_len, rnn_size)\n","            # => score has shape: (batch_size, 1, max_len)\n","            score = tf.matmul(decoder_output, self.wa(\n","                encoder_output), transpose_b=True) #(batch_size, 1, max_len)\n","        elif self.attention_func == 'concat':\n","            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n","            # Decoder output must be broadcasted to encoder output's shape first\n","            decoder_output = tf.tile(\n","                decoder_output, [1, encoder_output.shape[1], 1]) #shape (batch size, max len,hidden_dim)\n","\n","            # Concat => Wa => va\n","            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n","            score = self.va(\n","                self.wa(tf.concat((decoder_output, encoder_output), axis=-1))) # (batch_size, max len, 1)\n","\n","            # Transpose score vector to have the same shape as other two above\n","            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n","            score = tf.transpose(score, [0, 2, 1]) #(batch_size, 1, max_len)\n","\n","        # alignment a_t = softmax(score)\n","        alignment = tf.keras.activations.softmax(score, axis=-1) #(batch_size, 1, max_len)\n","\n","        # context vector c_t is the weighted average sum of encoder output\n","        context = tf.matmul(alignment, encoder_output) # (batch_size, 1, hidden_dim)\n","\n","        return context, alignment"],"metadata":{"id":"8O6gI5hR6Y8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ATTENTION_FUNC='general'"],"metadata":{"id":"w-P7zma76G0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EMBEDDING_DIM = 128\n","HIDDEN_DIM=1024 #512"],"metadata":{"id":"2lNjBZw45zrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the length of the input and output vocabulary\n","num_words_inputs = len(word2idx_inputs) + 1\n","num_words_output = len(word2idx_outputs) + 1\n","#Create the encoder\n","encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)\n","decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM, ATTENTION_FUNC)\n","\n","# Call the encoder and then the decoder\n","initial_state = encoder.init_states(1)\n","encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n","decoder_outputs = decoder(tf.constant(\n","    [[1]]), encoder_outputs[1:], encoder_outputs[0])"],"metadata":{"id":"0HUvscAd6J7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n","    ''' A training step, train a batch of the data and return the loss value reached\n","        Input:\n","        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n","            the input sequence\n","        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n","            the target seq, our target sequence\n","        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n","            the input sequence to the decoder, we use Teacher Forcing\n","        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n","            the initial state of the encoder\n","        - optimizer: a tf.keras.optimizers.\n","        Output:\n","        - loss: loss value\n","\n","    '''\n","    loss = 0.\n","    acc = 0.\n","    logits = None\n","\n","    with tf.GradientTape() as tape:\n","        en_outputs = encoder(input_seq, en_initial_states)\n","        en_states = en_outputs[1:]\n","        de_state_h, de_state_c = en_states\n","\n","        # We need to create a loop to iterate through the target sequences\n","        for i in range(target_seq_out.shape[1]):\n","            # Input to the decoder must have shape of (batch_size, length)\n","            # so we need to expand one dimension\n","            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n","            logit, de_state_h, de_state_c, _ = decoder(\n","                decoder_in, (de_state_h, de_state_c), en_outputs[0])\n","\n","            # The loss is now accumulated through the whole batch\n","            loss += loss_func(target_seq_out[:, i], logit)\n","            # Store the logits to calculate the accuracy\n","            logit = K.expand_dims(logit, axis=1)\n","            if logits is None:\n","                logits = logit\n","            else:\n","                logits = K.concatenate((logits,logit), axis=1)\n","        # Calculate the accuracy for the batch data\n","        acc = accuracy_fn(target_seq_out, logits)\n","    # Update the parameters and the optimizer\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","\n","    return loss / target_seq_out.shape[1], acc"],"metadata":{"id":"heAdP0iL6Q68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the main train function\n","def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n","\n","    losses = []\n","    accuracies = []\n","\n","    for e in range(n_epochs):\n","        # Get the initial time\n","        start = time.time()\n","        # Get the initial state for the encoder\n","        en_initial_states = encoder.init_states(batch_size)\n","        # For every batch data\n","        for batch, (input_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n","            # Train and get the loss value\n","            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n","\n","            if batch % 100 == 0:\n","                # Store the loss and accuracy values\n","                losses.append(loss)\n","                accuracies.append(accuracy)\n","                print('Epoch {} Batch {} Loss {:.4f} Acc:{:.4f}'.format(e + 1, batch, loss.numpy(), accuracy.numpy()))\n","\n","        # saving (checkpoint) the model every 2 epochs\n","        if (e + 1) % 2 == 0:\n","            checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","        print('Time taken for 1 epoch {:.4f} sec\\n'.format(time.time() - start))\n","\n","    return losses, accuracies"],"metadata":{"id":"pmJjiju46jEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_func(targets, logits):\n","    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True)\n","    # Mask padding values, they do not have to compute for loss\n","    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n","    mask = tf.cast(mask, dtype=tf.int64)\n","    # Calculate the loss value\n","    loss = crossentropy(targets, logits, sample_weight=mask)\n","\n","    return loss\n","\n","def accuracy_fn(y_true, y_pred):\n","    # y_pred shape is batch_size, seq length, vocab size\n","    # y_true shape is batch_size, seq length\n","    pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n","    correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n","\n","    # 0 is padding, don't include those\n","    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n","    n_correct = K.sum(mask * correct)\n","    n_total = K.sum(mask)\n","\n","    return n_correct / n_total"],"metadata":{"id":"aq4P7vir6veP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an Adam optimizer and clips gradients by norm\n","optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n","# Create a checkpoint object to save the model\n","checkpoint_dir = './training_ckpt_seq2seq_att'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","\n","losses, accuracies = main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4h9UI8e6isa","outputId":"6dd7c25b-9fe5-4246-a6af-26a416ceadf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 3.7963 Acc:0.0000\n","Epoch 1 Batch 100 Loss 2.6536 Acc:0.1296\n","Epoch 1 Batch 200 Loss 2.3875 Acc:0.1912\n","Epoch 1 Batch 300 Loss 2.5453 Acc:0.1577\n","Epoch 1 Batch 400 Loss 2.1477 Acc:0.1966\n","Epoch 1 Batch 500 Loss 2.4088 Acc:0.2261\n","Epoch 1 Batch 600 Loss 2.2102 Acc:0.2313\n","Epoch 1 Batch 700 Loss 2.5440 Acc:0.2407\n","Epoch 1 Batch 800 Loss 1.9171 Acc:0.2562\n","Epoch 1 Batch 900 Loss 2.0168 Acc:0.2648\n","Epoch 1 Batch 1000 Loss 2.1904 Acc:0.2453\n","Epoch 1 Batch 1100 Loss 2.0152 Acc:0.2857\n","Epoch 1 Batch 1200 Loss 1.7899 Acc:0.2911\n","Epoch 1 Batch 1300 Loss 2.0435 Acc:0.2530\n","Epoch 1 Batch 1400 Loss 2.3464 Acc:0.2828\n","Epoch 1 Batch 1500 Loss 1.9935 Acc:0.3098\n","Time taken for 1 epoch 627.8108 sec\n","\n","Epoch 2 Batch 0 Loss 1.8579 Acc:0.3095\n","Epoch 2 Batch 100 Loss 1.9261 Acc:0.2717\n","Epoch 2 Batch 200 Loss 1.7604 Acc:0.3162\n","Epoch 2 Batch 300 Loss 2.1833 Acc:0.3014\n","Epoch 2 Batch 400 Loss 1.5512 Acc:0.3511\n","Epoch 2 Batch 500 Loss 1.9463 Acc:0.3109\n","Epoch 2 Batch 600 Loss 1.9646 Acc:0.3248\n","Epoch 2 Batch 700 Loss 2.0350 Acc:0.3011\n","Epoch 2 Batch 800 Loss 1.9326 Acc:0.3409\n","Epoch 2 Batch 900 Loss 1.8026 Acc:0.3320\n","Epoch 2 Batch 1000 Loss 1.6921 Acc:0.3259\n","Epoch 2 Batch 1100 Loss 2.0444 Acc:0.3394\n","Epoch 2 Batch 1200 Loss 1.7827 Acc:0.3741\n","Epoch 2 Batch 1300 Loss 1.4355 Acc:0.3815\n","Epoch 2 Batch 1400 Loss 1.4996 Acc:0.4219\n","Epoch 2 Batch 1500 Loss 1.5221 Acc:0.3516\n","Time taken for 1 epoch 596.4092 sec\n","\n","Epoch 3 Batch 0 Loss 1.3203 Acc:0.4331\n","Epoch 3 Batch 100 Loss 1.2431 Acc:0.4650\n","Epoch 3 Batch 200 Loss 1.3533 Acc:0.4779\n","Epoch 3 Batch 300 Loss 1.3727 Acc:0.4296\n","Epoch 3 Batch 400 Loss 1.3064 Acc:0.4137\n","Epoch 3 Batch 500 Loss 1.5348 Acc:0.4008\n","Epoch 3 Batch 600 Loss 1.4932 Acc:0.3900\n","Epoch 3 Batch 700 Loss 1.5493 Acc:0.4259\n","Epoch 3 Batch 800 Loss 1.2622 Acc:0.4601\n","Epoch 3 Batch 900 Loss 1.6155 Acc:0.4345\n","Epoch 3 Batch 1000 Loss 1.1751 Acc:0.4935\n","Epoch 3 Batch 1100 Loss 1.2405 Acc:0.4667\n","Epoch 3 Batch 1200 Loss 1.3721 Acc:0.4453\n","Epoch 3 Batch 1300 Loss 1.3291 Acc:0.4577\n","Epoch 3 Batch 1400 Loss 1.3958 Acc:0.4307\n","Epoch 3 Batch 1500 Loss 1.3146 Acc:0.4697\n","Time taken for 1 epoch 595.7680 sec\n","\n"]}]},{"cell_type":"markdown","source":["**---------------------------------------------------------**"],"metadata":{"id":"bjYGuba37J7Q"}},{"cell_type":"code","source":["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","# plot some data\n","ax1.plot(losses, label='loss')\n","#plt.plot(results.history['val_loss'], label='val_loss')\n","ax1.set_title('Training Loss')\n","ax1.legend()\n","# accuracies\n","ax2.plot(accuracies, label='acc')\n","#plt.plot(results.history['val_accuracy_fn'], label='val_acc')\n","ax2.set_title('Training Accuracy')\n","ax2.legend()\n","plt.show()"],"metadata":{"id":"YtnqS3fT6ipc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an Adam optimizer and clips gradients by norm\n","optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n","# Create a checkpoint object to save the model\n","checkpoint_dir = './training_ckpt_seq2seq_att'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"metadata":{"id":"5y57utts7ezp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e884ec8d-7187-4c54-8a66-42f811a47fa4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f653feb5650>"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["def predict_seq2seq_att(input_text, input_max_len, tokenizer_inputs, word2idx_outputs, idx2word_outputs):\n","    if input_text is None:\n","        input_text = input_data[np.random.choice(len(input_data))]\n","    print(input_text)\n","    # Tokenize the input text\n","    input_seq = tokenizer_inputs.texts_to_sequences([input_text])\n","    # Pad the sentence\n","    input_seq = pad_sequences(input_seq, maxlen=input_max_len, padding='post')\n","    # Get the encoder initial states\n","    en_initial_states = encoder.init_states(1)\n","    # Get the encoder outputs or hidden states\n","    en_outputs = encoder(tf.constant(input_seq), en_initial_states)\n","    # Set the decoder input to the sos token\n","    de_input = tf.constant([[word2idx_outputs['<sos>']]])\n","    # Set the initial hidden states of the decoder to the hidden states of the encoder\n","    de_state_h, de_state_c = en_outputs[1:]\n","\n","    out_words = []\n","    alignments = []\n","\n","    while True:\n","        # Get the decoder with attention output\n","        de_output, de_state_h, de_state_c, alignment = decoder(\n","            de_input, (de_state_h, de_state_c), en_outputs[0])\n","        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n","        # Detokenize the output\n","        out_words.append(idx2word_outputs[de_input.numpy()[0][0]])\n","        # Save the aligment matrix\n","        alignments.append(alignment.numpy())\n","\n","        if out_words[-1] == '<eos>' or len(out_words) >= 20:\n","            break\n","    # Join the output words\n","    print(' '.join(out_words))\n","    return np.array(alignments), input_text.split(' '), out_words"],"metadata":{"id":"vLQQJYBF7j27"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Emoji analysis**"],"metadata":{"id":"k_BtPa-L4oPB"}},{"cell_type":"code","source":["!pip install emot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNhP9FXU4q3P","outputId":"3c804dc5-3364-4555-bef4-a17010be96ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 2.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 2.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 16 kB/s \n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n"]}]},{"cell_type":"code","source":["import re\n","import pickle\n","\n","with open('/content/drive/MyDrive/NLP/Emoji_Dict.p', 'rb') as fp:\n","    Emoji_Dict = pickle.load(fp)\n","Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n","\n","def convert_emojis_to_word(text):\n","    for emot in Emoji_Dict:\n","        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n","    return text"],"metadata":{"id":"t2UvkZix7qlU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Short forms / SMS Slang**"],"metadata":{"id":"pcvWMDQ97rcu"}},{"cell_type":"code","source":["messages = { \"AFAIK\" : \"As Far As I Know\",\n","\"AFK\" : \"Away From Keyboard\",\n","\"ASAP\" : \"As Soon As Possible\",\n","\"ATK\" : \"At The Keyboard\",\n","\"ATM\" : \"At The Moment\",\n","\"A3\" : \"Anytime, Anywhere, Anyplace\",\n","\"BAK\" : \"Back At Keyboard\",\n","\"BBL\" : \"Be Back Later\",\n","\"BBS\" : \"Be Back Soon\",\n","\"BFN\" : \"Bye For Now\",\n","\"B4N\" : \"Bye For Now\",\n","\"BRB\" : \"Be Right Back\",\n","\"BRT\" : \"Be Right There\",\n","\"BTW\" : \"By The Way\",\n","\"B4\" : \"Before\",\n","\"B4N\" : \"Bye For Now\",\n","\"CU\" : \"See You\",\n","\"CUL8R\" : \"See You Later\",\n","\"CYA\" : \"See You\",\n","\"FAQ\" : \"Frequently Asked Questions\",\n","\"FC\" : \"Fingers Crossed\",\n","\"FWIW\" : \"For What It's Worth\",\n","\"FYI\" : \"For Your Information\",\n","\"GAL\" : \"Get A Life\",\n","\"GG\" : \"Good Game\",\n","\"GN\" : \"Good Night\",\n","\"GMTA\":\"Great Minds Think Alike\",\n","\"GR8\" : \"Great!\",\n","\"G9\" : \"Genius\",\n","\"IC\" : \"I See\",\n","\"ICQ\" : \"I Seek you\",\n","\"ILU\": \"I Love You\",\n","\"ILY\": \"I Love You\",\n","\"IMHO\" : \"In My Honest/Humble Opinion\",\n","\"IMO\" : \"In My Opinion\",\n","\"IOW\" : \"In Other Words\",\n","\"IRL\" : \"In Real Life\",\n","\"KISS\" : \"Keep It Simple, Stupid\",\n","\"LDR\" : \"Long Distance Relationship\",\n","\"LMAO\" : \"Laugh My A.. Off\",\n","\"LOL\" : \"Laughing Out Loud\",\n","\"LTNS\" : \"Long Time No See\",\n","\"L8R\" : \"Later\",\n","\"MTE\" : \"My Thoughts Exactly\",\n","\"M8\" : \"Mate\",\n","\"NRN\" : \"No Reply Necessary\",\n","\"OIC\" : \"Oh I See\",\n","\"PITA\" : \"Pain In The A\",\n","\"PRT\" : \"Party\",\n","\"PRW\" : \"Parents Are Watching\",\n","\"QPSA?\" : \t\"Que Pasa?\",\n","\"ROFL\" : \"Rolling On The Floor Laughing\",\n","\"ROFLOL\" : \"Rolling On The Floor Laughing Out Loud\",\n","\"ROTFLMAO\" : \"Rolling On The Floor Laughing My A.. Off\",\n","\"SK8\" : \"Skate\",\n","\"STATS\" : \"Your sex and age\",\n","\"ASL\" : \"Age, Sex, Location\",\n","\"THX\" : \"Thank You\",\n","\"TTFN\" : \"Ta-Ta For Now!\",\n","\"TTYL\" : \"Talk To You Later\",\n","\"U\" : \"You\",\n","\"U2\" : \"You Too\",\n","\"U4E\" : \"Yours For Ever\",\n","\"WB\" : \"Welcome Back\",\n","\"WTF\": \"What The F...\",\n","\"WTG\" : \"Way To Go!\",\n","\"WUF\" : \"Where Are You From?\",\n","\"W8\"  : \"Wait...\"\n","}"],"metadata":{"id":"c3kW_qhE7vWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translates(sent):\n","  new_sent = \"\"\n","  words = sent.split(\" \")\n","  for word in words:\n","    if word.upper() in messages.keys():\n","      new_sent+= messages[word.upper()]\n","    else:\n","      new_sent+=word\n","    new_sent+=\" \"\n","  return new_sent"],"metadata":{"id":"CgFCq9ch7vsV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**UI - Flask APP**"],"metadata":{"id":"pvIhr3K78GQM"}},{"cell_type":"code","source":["!pip install flask-ngrok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXJavH6z8JAR","outputId":"6efb4ff3-53e5-48b6-e30e-a065baf3d871"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.10.8)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","source":["!pip install pyngrok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--ImCJaS8LSN","outputId":"d1dd5638-2d8e-49d6-ad83-f462233e3474"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n","\u001b[K     |████████████████████████████████| 745 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19006 sha256=e65f1ada28909cd30da29de2c2ab83ccd5f215ccd98d87906458cf3f84730ee8\n","  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-5.1.0\n"]}]},{"cell_type":"code","source":["!ngrok authtoken \"21zLoyqOXDMBV3J0A0UA2TUIJKt_3unqjxyfWKpbXu6A629ke\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVxBMmK88L4B","outputId":"d66544ca-1e4f-4142-e910-ee96181598c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"]}]},{"cell_type":"code","source":["!pip install codeswitch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjujuBJPJDN1","outputId":"50fa91d7-fdb8-4df8-adca-18a632862d7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting codeswitch\n","  Downloading codeswitch-1.1-py3-none-any.whl (4.1 kB)\n","Collecting transformers\n","  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->codeswitch) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->codeswitch) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->codeswitch) (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->codeswitch) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->codeswitch) (4.62.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 437 kB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 43.3 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 38.3 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 37.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->codeswitch) (3.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->codeswitch) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers->codeswitch) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->codeswitch) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->codeswitch) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->codeswitch) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->codeswitch) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->codeswitch) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->codeswitch) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->codeswitch) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->codeswitch) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->codeswitch) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, codeswitch\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed codeswitch-1.1 huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0\n"]}]},{"cell_type":"code","source":["!pip install translator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WxfPwQmC__-A","outputId":"38b64919-d5a0-430b-898d-7fbdff42d072"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting translator\n","  Downloading translator-0.0.9-py3-none-any.whl (5.3 kB)\n","Requirement already satisfied: beautifulsoup4>=4.0 in /usr/local/lib/python3.7/dist-packages (from translator) (4.6.3)\n","Installing collected packages: translator\n","Successfully installed translator-0.0.9\n"]}]},{"cell_type":"code","source":["!pip install translate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Nd3rjWJFHgc","outputId":"35ea77bd-86be-461d-afb7-9df7f3e36419"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting translate\n","  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from translate) (2.23.0)\n","Collecting libretranslatepy==2.1.1\n","  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from translate) (7.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from translate) (4.2.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (3.0.4)\n","Installing collected packages: libretranslatepy, translate\n","Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"]}]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rha9DIQEABzA","outputId":"edafe876-1e24-4985-9475-c5bfaef09f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}]},{"cell_type":"code","source":["from flask import Flask, render_template, request\n","from flask_ngrok import run_with_ngrok\n","from translate import Translator\n","from codeswitch.codeswitch import NER\n","from codeswitch.codeswitch import POS\n","from codeswitch.codeswitch import LanguageIdentification\n","\n","from nltk.translate.bleu_score import sentence_bleu\n","app = Flask(__name__, template_folder='/content/drive/MyDrive/NLP/templates',\n","            static_folder='/content/drive/MyDrive/NLP/static/images')\n","run_with_ngrok(app)\n","\n","\n","@app.route(\"/\")\n","def home():\n","  return render_template(\"main.html\")\n","\n","@app.route(\"/analytics\")\n","def graph():\n","    return render_template(\"displayGraph.html\")\n","\n","@app.route(\"/mainFeatures\")\n","def feature():\n","    return render_template(\"mainFeatures.html\")\n","\n","@app.route(\"/translate\")\n","def translateText():\n","    return render_template(\"translate.html\")\n","\n","@app.route(\"/langResult\", methods=['POST'])\n","def trans():\n","  res_source = \"\"\n","  res_dest = \"\"\n","  test_sent = \" \"\n","  sent = request.form[\"text\"]\n","  test_sent = convert_emojis_to_word(sent)\n","  test_sent = translates(sent)\n","  alignments, source, prediction = predict_seq2seq_att(sent, input_max_len, tokenizer_inputs,\n","                                                     word2idx_outputs, idx2word_outputs)\n","  for x in source:\n","    res_source +=x\n","  for y in prediction:\n","    res_dest+=y\n","    res_dest+=\" \"\n","\n","  return test_sent+ \" ** \" + res_dest\n","\n","@app.route(\"/shortform\")\n","def show():\n","    return render_template(\"emoShortForm.html\")\n","\n","@app.route(\"/esf\", methods=[\"POST\"])\n","def esf():\n","    my_sent = request.form['text']\n","    user_ip = my_sent\n","    my_sent = convert_emojis_to_word(my_sent)\n","    my_sent = translates(my_sent)\n","    return render_template(\"smsShow.html\", text=user_ip, answer=my_sent)\n","\n","@app.route(\"/check\")\n","def check():\n","    return render_template(\"Bleu.html\")\n","\n","@app.route(\"/calculate\", methods=['POST'])\n","def calculate():\n","    text = request.form['text']\n","    original_list = [text.split(\" \")]\n","    translator= Translator(from_lang=\"hindi\",to_lang=\"english\")\n","    translation = translator.translate(text)\n","    destination_list = translation.split(\" \")\n","    score = sentence_bleu(original_list, destination_list)\n","    return render_template(\"result.html\", score=score, inter=translation, text=text)\n","\n","@app.route(\"/bot\")\n","def bot():\n","    return render_template(\"bot.html\")\n","\n","@app.route(\"/ner\")\n","def ner():\n","    return render_template(\"ner.html\")\n","\n","@app.route(\"/nerresult\", methods=[\"POST\"])\n","def nerresult():\n","    ner = NER('hin-eng')\n","    txt = request.form['text']\n","    result = ner.tag(txt)\n","    return render_template(\"nerResult.html\", result=result)\n","\n","@app.route(\"/language\")\n","def postag():\n","    return render_template(\"pos.html\")\n","\n","\n","@app.route(\"/posresult\", methods=[\"POST\"])\n","def posres():\n","    text = request.form['text']\n","    pos = POS('hin-eng')\n","    result = pos.tag(text)\n","    return render_template(\"posResult.html\", result=result)\n","\n","@app.route(\"/identify\")\n","def ident():\n","    return render_template(\"langIdentify.html\")\n","\n","@app.route(\"/langresult\", methods=[\"POST\"])\n","def langres():\n","    text = request.form['text']\n","    lid = LanguageIdentification('hin-eng')\n","    result = lid.identify(text)\n","    return render_template(\"langResult.html\", result=result)\n","\n","app.run()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2l-56He8V7D","outputId":"2674945e-f209-4d58-b0e9-c7301cbee2ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://1543-34-80-4-208.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Dec/2021 19:55:17] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [09/Dec/2021 19:55:18] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [09/Dec/2021 19:55:21] \"\u001b[37mGET /translate HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["mujhe morning mai ghoomna bahut pasand hai\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [09/Dec/2021 19:55:28] \"\u001b[37mPOST /langResult HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["i m going to see a lot of a lot of i . <eos>\n"]}]}]}